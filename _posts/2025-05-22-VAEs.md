---
title: "Toward Deep Generative Models - Variational Auto Encoder Part 1."
date: 2025-05-22
last_modified_at: 2025-05-22
categories:
  - Note
tags:
  - Generative Model
  - Machine Learning
math: true
---
# Variational Auto Encoders (VAEs)
ðŸ“Œ *Note:* This is a series of personal study note for the course CS-E5890 Deep Generative Model, written to help me clarify and explain topics in Deep Generative Models to myself (or anyone is reading). It reflects my own understanding and reasoning, not a formal guide nor a tutorial or authoritative reference. 
## Latent Variable Models
One inevitable concept in generative model is latent variables. So what is latent variables, for my own understanding, any "lower dimension" representation of the hidden structure, causes, or underlying factors that explain the observed data which is not observable. One simple example of a latent variable model is factor analysis.
### Factor Analysis (FA)
We assume that we have observed data $D$, and we further assume that there are some implicit relation between the data $D$ causing what it looks like what we saw, so we define a loading factor $F$ which capture this relation. Here in factor analysis, we assume the relation of the factor and the data are linear, then we can further add some bias and noise resulting:

$$
D=F+C+\epsilon
$$

where $\epsilon \sim \mathcal{N}(0, \Psi)$. The goal is to learn $F$, $C$, and the noise covariance $\Psi$. However, all variables are unknown, so we introduce a latent variable $h$, which captures the relations between variables in the loading vector. We assume $h \sim \mathcal{N}(0, I)$, resulting in:

$$
D = Fh + C + \epsilon.
$$

With the latent variable, we can write the likelihood as $p(D \mid h) = \mathcal{N}(Fh + C, \Psi)$. Once we have the likelihood, we can use various statistical models to learn the parameters, such as the [EM algorithm](https://henryhsu0217.github.io/note/EM/) or variational inference.e.

After learning of the variables, one can now analysis the relation, composition of the data by analyzing the loading factor. But what is more interesting here, is that with the learned variables, we are able to:
- Using the posterior $p(h \mid D) = \frac{p(D \mid h)\, p(h)}{p(D)}$ to infer the hidden, lower-dimensional latent variable that represents the data.
- Generate new data $D$ with new sampled latent variable.

The second point is then the intuition of latent generative models, since the latent $h$ now is this setup have a prior of $\mathcal{N}(0, \Psi)$, so sampling a latent variable and feed back to our FA model will yield new data in the $D$ space.

## Variational Auto Encoder 
With the idea of latent model we can now look into VAEs, but first we can briefly go through auto encoders (AEs).
### Auto Encoders
AEs are a neural network structure that learns to encode, compress data and decode it back to the original data. As in [Fig. 1](#figae), there are 2 main components, a encoder and a decoder, which the encoder encodes the input to a lower dimension feature representation, and the decoder try to recover, decode the feature back to its origin. Then the training is basically the loss of the decoder's output and the original data, and passes the gradient all the way through all components.

<a id="figae"></a>![AE](/assets/images/VAE/AE.png)
*Illustration of Auto Encoder [[1]](#ref_1)*

With the basic AEs, we can start working toward VAEs. So now the AE is just encodes data and restore the data, no new data is generating, but here, we can link the decoder part to the FA discussed above, the loading factor, bias can basically seen as the the parameters in the decoder neural networks, and the encoded feature is then the latent variables.

### Intuition
Then one could wonder if the decoder is already in the setup of FA, then why an encoder? Since in FA, the main goal is finding the factor, bias, and noise, although it is able to generate new data as mentioned, but there is no training nor alignment of the generated data. So we could keep sampling new latent and generate data but the generated data my not make sense nor met our desire, therefore a encoder must be trained to encode the desired data into specific latent space distribution, then we sample the latent from that distribution for generation.

### Math 
Then the maths, so the encoder needs to encode the data into a latent space distribution that can represent the data, this can easily done by setting the output of the encoder to be mean $\mu$ and variance $\sigma$ of the distribution, and the decoder needs to decoder the latent to generate new data. So by first defining the input data to be $x$ and latent to be $z$.

Then we define the decoder as $p_\theta(x|z)$, which is the likelihood of data $x$ to be generated by latent $z$. Then we are interested in the posterior distribution, which means the distribution of the latent space after seeing the data $x$:

$$
p_{\theta}(z \mid x) = \frac{p_{\theta}(x \mid z)\, p(z)}{p_{\theta}(x)}
$$

where the prior $p(z)$ can be the initial distribution of the latent space, normally $\mathcal{N}(0,I)$. Unlike FA, the evidence term $p_\theta(x)$ is intractable, but we can approximate $p_\theta(z|x)$ with $q(z|x)$ using variational inference.

To approximate the posterior, we want the approximation of $q(z|x)$ to be as close to $p_\theta(z|x)$, here Kullbackâ€“Leibler (KL) divergence can be used as the objective function, and the approximation of $q(z|x)$ can be seen as a neural network parameterized by $\psi$ resulting:

$$
\psi^\star = \arg\min_{\psi} D_{\text{KL}}\left(q_{\psi}(z \mid x) \,\|\, p_{\theta}(z \mid x)\right)
$$

a verbal summarize of the equation: We want a network to learn the parameter $\psi$ which defines the posterior of the latent distribution $q_{\psi}(z|x)$ that is close to the true posterior latent distribution given the data.

But as already stated, the evidence of the posterior is not tractable, so what makes the difference of using the KL and $q_{\psi}(z|x)$ to approximate it? Things lies in the KL divergence, here we can use the definition of the KL divergence and open it:

$$
\begin{align}
D_{\text{KL}}(q_{\psi}(z \mid x) \,\|\, p_{\theta}(z \mid x)) 
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \frac{q_{\psi}(z \mid x)}{p_{\theta}(z \mid x)}\right] \\
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log q_{\psi}(z \mid x) - \log p_{\theta}(z \mid x)\right] \\
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log q_{\psi}(z \mid x)\right] - \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log p_{\theta}(z \mid x)\right] \\
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log q_{\psi}(z \mid x)\right] - \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \left(\frac{p_{\theta}(x \mid z)p(z)}{p_{\theta}(x)}\right)\right] \\
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log q_{\psi}(z \mid x)\right] - \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \left(p_{\theta}(x \mid z)p(z)\right)\right] + \log p_{\theta}(x) \\
&= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \frac{q_{\psi}(z \mid x)}{p_{\theta}(x \mid z)p(z)}\right] + \log p_{\theta}(x) \\
\log p_{\theta}(x) 
&= \underbrace{\mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \frac{p_{\theta}(x \mid z)p(z)}{q_{\psi}(z \mid x)}\right]}_{\text{ELBO}} + D_{\text{KL}}(q_{\psi}(z \mid x) \,\|\, p_{\theta}(z \mid x)).
\end{align}
$$

From the final equation, since $\log p_\theta(x)$ is known and fixed during training, then if we maximize the second term it equals to minimizing the KL divergence, and the second term is also called the Evidence Lower Bound (ELBO). And we can further extend the ELBO as:

$$
\mathbb{E}_{q_{\psi}(z \mid x)}\left[\log \frac{p_{\theta}(x \mid z)\, p(z)}{q_{\psi}(z \mid x)}\right] 
= \mathbb{E}_{q_{\psi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right] 
- D_{\text{KL}}(q_{\psi}(z \mid x) \,\|\, p(z))
$$

which the first term says that we want the expected probability of generated data with sampled latent to be close as the original data as possible, and the second term regularize the encoded latent prior to learn the latent distribution representing the data but not too far away from the Normal distribution prior, [Fig. 2](#figae) from [[2]](#ref_2) shows the idea (the notation is different). 

<a id="figae"></a>![AE](/assets/images/VAE/VAEs.png)

This basically forms the loss function during training, however, considering how gradient are passed and flow through models, some further trick need to be used, which will be explained in future notes. At this moment, we already walk through the intuition of VAEs and latent models, next note will about VAE will be how to train, and build a VAE.

## Reference
<a id="ref_1"></a>[1]: K. A. Alaghbari, H.-S. Lim, M. H. M. Saad, and Y. S. Yong, "Deep autoencoder-based integrated model for anomaly detection and efficient feature extraction in IoT networks," *IoT*, vol. 4, no. 3, pp. 345â€“365, 2023. [Online]. Available: https://doi.org/10.3390/iot4030016

<a id="ref_2"></a>[2]: S. J. D. Prince, Understanding Deep Learning, MIT Press, 2023.

