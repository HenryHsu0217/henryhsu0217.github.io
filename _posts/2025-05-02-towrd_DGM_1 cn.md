---
title: "走進深度生成模型的世界 —— 入門解析"
date: 2025-05-02
last_modified_at: 2025-05-02
categories:
  - Post
  - Chinese
  - 中文
tags:
  - Generative Model
  - Machine Learning
math: true
---
> 📌 *備註：* 這是一系列個人學習筆記，撰寫目的是幫助我自己釐清並解釋深度生成模型（Deep Generative Models）相關主題（也希望對正在閱讀的你有所幫助）。這些內容反映我個人的理解與思考，並非正式教學指南或權威參考資料。並且這是由Chat-GPT直接從我寫的英文版翻譯過來的，至於為什麼，因爲我懶得再重寫一遍而且我中打慢的有剩。

## 爆炸性成長
回到 2023 年初，當我第一次接觸到 OpenAI 的 GPT-3.5 時，我對這項新技術感到震驚，並迅速在日常生活與學校作業中大量使用它，世界各地的人也有相同的反應。就像其他普通人一樣，我把它當成黑盒使用，如果它產出的內容不符合我的期望，我就會覺得這模型很差勁，甚至開始怪它（現在回想起來有點可笑）。

然而，隨著生成模型逐漸成為大眾的常見技術並受到矚目且爆炸性成長，許多自媒體或個人開始盲目跟風，對 AI 推崇備至。每當某家大型科技公司推出新模型，就會有人熱烈宣傳這些模型有多厲害、多酷，卻根本不了解它們的運作原理。我不想成為那樣的人。

因此，我開始學習資料科學，隨著深入研究，我越來越好奇生成模型或通用人工智慧的發展潛力。本篇文章將會簡單介紹什麼是生成模型、它們實際在做什麼，讓它不再只是一個黑盒。後續的文章將會針對各種模型進行更深入的探討。

## 生成模型到底在做什麼？
對於像 GPT 這樣的 Large Language Models（LLMs）來說，表面上看起來它們像是能夠思考並根據我們的輸入做出正確回應的 AI，似乎和「生成」沒什麼關係。但其實，LLMs 和圖像、語音、影片等生成模型在本質上是一樣的。簡單來說，LLMs 的回應是「一個詞一個詞地生成的」，更精確地說，是以 token 為單位。模型根據輸入與先前生成的 token 來預測下一個 token，逐步組成我們看到的文字回應。這個流程在日後介紹 Transformer 的文章中會更詳細說明。不過總結來說，LLMs 也是一種生成模型。

那麼，生成模型到底在做什麼？以我自己的話來說，就是「學習我們想要產生的資料的機率分布，然後從這個分布中抽樣出新資料」。這聽起來有點抽象也難以理解，所以我們來舉個例子。

先以 LLM 為例。前面說過：「模型根據輸入與先前的 token 來生成下一個 token」，假設現在我們有一個 token 序列是："Hi"、","、"my"、"name"、"is"、"Henry"，接下來模型的任務就是在所有可能的回應 token（也就是整個詞彙表、數字或標點符號等）中，根據目前輸入，學習下一個 token 的機率分布。這可以從下圖中看出。
![Illustration of how LLMs work.](/assets/images/TDGM-1/DGM_EX1.png)*Illustration of how LLMs work.*

更數學化一點，LLM 學習的是 $p(next\_token|prev\_token)$，然後依據這個機率來生成下一個 token。類似地，在圖像生成的情境中，我們希望模型能學習某個特定圖像類別的分布，然後從該類別中抽樣生成新圖像。

所以總結來說，生成模型的本質就是「機率建模」（probabilistic modeling）：在給定條件下，學習目標資料的機率分布，然後從中抽樣來生成新資料。而當我一開始接觸這個領域時，我的疑問是：為什麼要用機率？為什麼不像傳統機器學習或深度學習那樣，讓模型學習如何輸出我們想要的確切資料（例如輸出一張圖或一句話）？

後來才發現答案其實很直觀：如果模型只學習產生「特定」的輸出，它就會失去多樣性，變成「還原模型」（reverting model），也就是根據條件直接反推出一個固定的輸出。這樣就會喪失生成能力，變得像 Autoencoder 那樣的模型了。

## 結語
這篇文章簡單介紹了生成模型背後的運作邏輯，並透過一些例子幫助理解。在接下來的文章中，我會開始探討最基礎的生成模型，如 VAE、Diffusion、Transformer，並介紹機率建模的一些基本數學概念。
