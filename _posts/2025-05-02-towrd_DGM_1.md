---
title: "Toward Deep Generative Models â€” Introduction"
date: 2025-05-02
last_modified_at: 2025-05-02
categories:
  - Note
tags:
  - Generative Model
  - Machine Learning
math: true
---
> ðŸ“Œ *Note:* This is a series of personal study note for the course CS-E5890 Deep Generative Model, written to help me clarify and explain topics in Deep Generative Models to myself (or anyone is reading). It reflects my own understanding and reasoning, not a formal guide nor a tutorial or authoritative reference. 

## The Explosion
So back in early 2023, when I first experienced OpenAI's GPT-3.5 I was shocked by this new technology, and rapid abused it in daily life and school work, and so is the world. Just as other normal person, I treat it as a black box and get upset if it didn't generate the output that match my expectation, and start blaming the model (thinking from now it seems stupid). 

Nonetheless, as generative model start being a common technology to the public and gaining spot lights, a lot of self-media and individual started to spread content blindly worship and follow the trend of AIs, when any big tech company published a new model, they will be sharing how good how cool it is and being hype about it, despite not knowing how it works, and I do not want to be that kind of people.

So I started my studies in data science, and as I deepen into the topic, now I become wondering how far can generative model or general AI goes. Therefore, this post will give an introduction to what exactly is generative model doing, making it not just a black box, then there will be other post about different model in detail.

## What are generative models doing?
 For Large Language Models (LLMs) like GPTs, looks like they are AIs that are able to think, and decide the correct response according to our input, and have nothing to do with "generating". However, LLMs are identical to other generative model such as image, voice, and video generation. Here in short, LLMs' response is generated word by word, or in more specific, tokens. The model generates the next tokens according to the input and previous generated tokens, forming text response to the user, which will be explained more deeply in future post about Transformers, but overall, LLMs are also in forms of generative model.

So, what are generative models doing? With my own word, I would say it is learning the data distribution of our desired data output, then sample from that distribution. It sounds abstract and hard to understand, but let's start with some example.

Let's first take LLMs as example. As said earlier, "The model generates the next tokens according to the input and previous generated tokens", so if we now have the following sequence of tokens as "Hi", ",", "my", "name", "is", "Henry", and we have all the possible response token, which in this case, all possible vocabulary, numbers, or punctuation, as our data, then the model's goal is to learn the distribution of the data for the next token, which is illustrated in the image bellow.
![Illustration of how LLMs work.](/assets/images/TDGM-1/DGM_EX1.png)*Illustration of how LLMs work.*

More mathematically, the LLMs learn $p(next\_token|prev\_token)$ and then generate the next token according to the probability. Similar to LLMs, when we look at image generation, we want the model to able to learn the distribution of desired image class, and then sample from that class to generate images.

So, overall, what generative models are doing is just probabilistic modeling, with given condition, it needs to learn the probability distribution of the desired data and samples from it to generate new data. And one question when I first look into this realm is that why probability? Why not as normal machine or deep learning to learn to generate the exact image or response (GPTs) we want? Well it turn out the answer for this question is straight forward, if we let the model learn to generate exact data, then it losses diversity, and become a reverting model, which reverse your condition to a exact data, therefore losses its generation capability, becoming a auto encoder like model.

## Conclusion
This post now gave a quick walk through of what generative models are doing in the background and gave some simple example. In upcoming posts, I'll go through the most fundmental generative models such as VAE, diffusion, transformer, and also some basic math for probabilistic modeling.

